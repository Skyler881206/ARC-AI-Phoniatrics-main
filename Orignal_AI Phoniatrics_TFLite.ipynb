{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "from multiprocessing import Process, Pool\n",
    "from random import shuffle\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 32\n",
    "PATH = '/home/power703/work/cgh/data/student/'\n",
    "IN_PATH = []\n",
    "\n",
    "def save_spectrum_to_npy(wavfile):\n",
    "    import librosa\n",
    "    y, _ = librosa.load(wavfile)\n",
    "    S = np.abs(librosa.stft(y, n_fft=512))\n",
    "    p = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    tmp = np.zeros([256, 128])\n",
    "    if p.shape[1] > 128:\n",
    "        tmp[:256, :128] = p[:256, :128]\n",
    "    else:\n",
    "        tmp[:256, :p.shape[1]] = p[:256, :p.shape[1]]\n",
    "    tmp = (tmp+40)\n",
    "    tmp = tmp/40.0\n",
    "    np.save(wavfile[:-4]+'.npy', tmp)\n",
    "\n",
    "def save_mfcc_to_npy(wavfile):\n",
    "    import librosa\n",
    "    y, sr = librosa.load(wavfile)\n",
    "    data_mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=32)\n",
    "\n",
    "    tmp = np.zeros([32, 64])\n",
    "    if data_mfcc.shape[1] > 64:\n",
    "        tmp[:32, :64] = data_mfcc[:32, :64]\n",
    "    else:\n",
    "        tmp[:32, :data_mfcc.shape[1]] = data_mfcc[:64, :data_mfcc.shape[1]]\n",
    "    np.save(wavfile[:-4]+'.npy', tmp)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for root2, dirs2, files2 in os.walk(os.path.abspath(PATH)):\n",
    "        for file2 in files2:\n",
    "            if('wav' in file2):\n",
    "                IN_PATH.append(os.path.join(root2, file2))\n",
    "    Pool(num_threads).map(save_mfcc_to_npy, IN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 64\n",
    "dataset = 'student'\n",
    "ROOT = '/work/power703/cgh/data/'\n",
    "save_path = ROOT+dataset+'/'\n",
    "classes = []\n",
    "ori_path = ROOT+dataset+'/ori/'\n",
    "aug_path = ROOT+dataset+'/aug/'\n",
    "\n",
    "for n, _, _ in os.walk(os.path.abspath(ori_path)):\n",
    "    classes.append(n.split('/')[-1])\n",
    "\n",
    "classes = classes[1:]\n",
    "classes = [int(x) for x in classes]\n",
    "classes.sort()\n",
    "\n",
    "classes = [str(x) for x in classes]\n",
    "\n",
    "numberOfPart = 5\n",
    "\n",
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_data(npy_name):\n",
    "    lable = npy_name.split('/')\n",
    "    lable = classes.index(lable[-2])\n",
    "    data = np.load(npy_name[:-4]+'.npy', allow_pickle=True)\n",
    "    return data, lable\n",
    "\n",
    "\n",
    "train_list = [[] for i in range(numberOfPart)]\n",
    "test_list = [[] for i in range(numberOfPart)]\n",
    "origin_list = []\n",
    "\n",
    "for n in classes:\n",
    "    ori_path = ROOT+dataset+'/ori/' + str(n)+'/'\n",
    "    ori_n = 0\n",
    "\n",
    "    for root2, _, files2 in os.walk(os.path.abspath(ori_path)):\n",
    "        for file2 in files2:\n",
    "            if('npy' in file2):\n",
    "                origin_list.append(os.path.join(root2, file2))\n",
    "                ori_n = ori_n+1\n",
    "\n",
    "origin_list = random.sample(origin_list, len(origin_list))\n",
    "origin_list = chunkIt(origin_list, numberOfPart)\n",
    "aug_list = [[] for i in range(numberOfPart)]\n",
    "\n",
    "for i in range(len(origin_list)):\n",
    "    tmp_list = [y[:-4] for y in origin_list[i]]\n",
    "    tmp_list = [y[y.rfind('/')+1:] for y in tmp_list]\n",
    "    for root1, _, files1 in os.walk(os.path.abspath(aug_path)):\n",
    "        for file1 in files1:\n",
    "            if('npy' in file1):\n",
    "                myname = file1[:file1.rfind('_')]\n",
    "                myname = myname[:myname.rfind('_')]\n",
    "                if(myname in tmp_list):\n",
    "                    aug_list[i].append(os.path.join(root1, file1))\n",
    "\n",
    "number_ori = np.zeros(len(classes))\n",
    "\n",
    "for data in origin_list[0]:\n",
    "    lable = data[data.rfind('/')+1:].split('_')\n",
    "    lable = classes.index(lable[0])  # +'_'+lable[1]\n",
    "    number_ori[lable] = number_ori[lable]+1\n",
    "\n",
    "\n",
    "for n in range(numberOfPart):\n",
    "    tmp_list = []\n",
    "    aug_list[n] = random.sample(aug_list[n], len(aug_list[n]))\n",
    "    number_aug = np.zeros(len(classes))\n",
    "    for data in aug_list[n]:\n",
    "        lable = data[data.rfind('/')+1:].split('_')\n",
    "        lable = classes.index(lable[0])  # +'_'+lable[1]\n",
    "        if (number_aug[lable] < (1500 - number_ori[lable])):  # or True\n",
    "            number_aug[lable] = number_aug[lable]+1\n",
    "            tmp_list.append(data)\n",
    "        else:\n",
    "            pass\n",
    "    aug_list[n] = tmp_list\n",
    "\n",
    "\n",
    "for select_part in range(numberOfPart):\n",
    "    for other_part in range(numberOfPart):\n",
    "        if select_part != other_part:\n",
    "            train_list[select_part] = train_list[select_part] + \\\n",
    "                origin_list[other_part]   + aug_list[other_part]\n",
    "        else:\n",
    "            test_list[select_part] = origin_list[select_part]\n",
    "\n",
    "\n",
    "pool = Pool(num_threads)\n",
    "for n in range(numberOfPart):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    number_list = np.zeros(len(classes))\n",
    "    tmp_list = []\n",
    "    for data in test_list[n]:\n",
    "        lable = data[data.rfind('/')+1:].split('_')\n",
    "        lable = classes.index(lable[0])  # +'_'+lable[1]\n",
    "        if (number_list[lable] < 82):  # or True\n",
    "            number_list[lable] = number_list[lable]+1\n",
    "            tmp_list.append(data)\n",
    "    test_list[n] = tmp_list\n",
    "\n",
    "    pool_outputs = pool.map(load_data, train_list[n])\n",
    "    for i in pool_outputs:\n",
    "        x_train.append(i[0])\n",
    "        y_train.append(i[1])\n",
    "\n",
    "    pool_outputs = pool.map(load_data, test_list[n])\n",
    "    for i in pool_outputs:\n",
    "        x_test.append(i[0])\n",
    "        y_test.append(i[1])\n",
    "\n",
    "    np.savez(\n",
    "        save_path+'part_'+str(n),\n",
    "        x_train=np.asarray(x_train),\n",
    "        y_train=np.asarray(y_train),\n",
    "        x_test=np.asarray(x_test),\n",
    "        y_test=np.asarray(y_test)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 4\n",
    "BATCH_SIZE = 32\n",
    "epochs = 100\n",
    "load_last = False\n",
    "dataset = 'student'\n",
    "model_name = 'MobileNetV3Small'\n",
    "s_n='0'\n",
    "INPUT_X=32\n",
    "INPUT_Y=64\n",
    "\n",
    "NAME = model_name+'_'+dataset+'_c'+str(classes)+'_p'+str(s_n)+'_bs'+str(BATCH_SIZE)+'_data_' + \\\n",
    "    datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "SAVE_PATH = '/work/power703/cgh/weight/'\n",
    "log_dir = '/work/power703/cgh/logs/' + NAME\n",
    "WEIGHT_PATH = '/work/power703/cgh/data/' + dataset + '/part_'+s_n+'.npz'\n",
    "\n",
    "LOAD_WEIGHT = '/work/power703/cgh/weight/DenseNet201_chu_7_c2_p0_bs32_data_0506_1054.14-0.67-0.99.hdf5'\n",
    "class_weight = {}\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = tf.keras.applications.MobileNetV3Small(\n",
    "        input_shape=(INPUT_X, INPUT_Y, 1),\n",
    "        weights=None,\n",
    "        classes=classes\n",
    "    )\n",
    "    if load_last:\n",
    "        model.load_weights(LOAD_WEIGHT)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "\n",
    "    x = np.load(WEIGHT_PATH, mmap_mode='r', allow_pickle=True)\n",
    "    x_train = x['x_train']\n",
    "    y_train = x['y_train']\n",
    "    x_test = x['x_test']\n",
    "    y_test = x['y_test']\n",
    "\n",
    "    total = len(y_train)\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    class_weight = dict(zip(unique, counts))\n",
    "    for l in class_weight:\n",
    "        w = class_weight[l]\n",
    "        new = (1 / w)*(total)/2.0\n",
    "        class_weight.update({l: new})\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], INPUT_X, INPUT_Y, 1)\n",
    "    y_train = to_categorical(y_train, num_classes=classes)\n",
    "\n",
    "    x_test = x_test.reshape(x_test.shape[0], INPUT_X, INPUT_Y, 1)\n",
    "    y_test = to_categorical(y_test, num_classes=classes)\n",
    "\n",
    "    SHUFFLE_BUFFER_SIZE = len(x_train)\n",
    "    return(\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            (x_train, y_train)).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE),\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            (x_test, y_test)).batch(BATCH_SIZE)\n",
    "    )\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = get_compiled_model()\n",
    "\n",
    "train_dataset, test_dataset = get_dataset()\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "train_dataset = train_dataset.with_options(options)\n",
    "test_dataset = test_dataset.with_options(options)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir=log_dir),\n",
    "        ModelCheckpoint(\n",
    "            SAVE_PATH + NAME + '.{epoch:02d}-{val_accuracy:.2f}-{val_loss:.2f}.hdf5',\n",
    "            monitor='val_accuracy',\n",
    "            verbose=2, save_best_only=True\n",
    "        ),\n",
    "    ],\n",
    "    validation_data=test_dataset,\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_WEIGHT = '/home/power703/work/cgh/weight/MobileNetV3Small_student_c4_p4_bs32_data_0523_1643.86-0.80-1.26.hdf5'\n",
    "OUTPUT_MODEL = '/work/power703/cgh/weight/'\n",
    "TFL_PATH = '/work/power703/cgh/tflite/'+'MobileNetV3Small_student_c4_p4'+'.tflite'\n",
    "\n",
    "model = tf.keras.models.load_model(LOAD_WEIGHT)\n",
    "tflite_models_dir = pathlib.Path(\"/home/power703/work/cgh/tflite\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "resnet_quantized_tflite_file = tflite_models_dir / \\\n",
    "    \"MobileNetV3Small_student_c4_p4_quantized.tflite\"\n",
    "resnet_quantized_tflite_file.write_bytes(converter.convert())\n"
   ]
  }
 ]
}
